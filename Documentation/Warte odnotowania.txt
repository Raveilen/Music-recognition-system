Sensowne technologie:
	- ACRCloud - biblioteka slużąca do identyfikowania treści (w tym między innymi muzyki)
	- NAudio - do przetwarzania dźwięku w C#

	- TPL - zrównoleglanie procesu (wielowątkowość przetwarzania dźwięku)

SQL Server jako baza danych dla aplikacji

Teoria Nyquist-Shannona	

Nagrywanie dźwięku w języku C#

Założenia dla systemu:
	- Pobrana próbka dźwięku będzie analizowana w domenie częstotliwości. Musimy zatem pobrać próbkę pozyskaną w domenie czasu i za pomocą dyskretnej transformaty Fourriera przenieść ją do dziedziny częstotliwości
	- Aby mieć punkt odniesienia dla aplikacji, dobrym pomysłem będzie stworzenie bazy danych przechowującej informacje o utworach. Zakładamy 3 podstawowe tabele:
		- Utwory
		- Fingerprint dla poszczególnych utowrów
		- Autorów (opcjonalnie ale może być przydatne w późiejszym etapie rozwoju czy do analiz)
		- Gatunek (tabela również opcjonalna i również do statystyk, wymagałaby ode mnie posortowania utowrów kategoriami)
	- Trzeba zastanowić się nad pipelinem do przetwarzania utworu:
	- Stworzenie systemu nagrywania dźwięku (jak starczy na to czasu) w celu dalszej analizy albo system udostępniania plików tak by możliwa była analiza na podstawie dostarczonej przez użytkownika treści.
	- Mechanizm udostępniania w aplikacji własnych utworów z opisami wiąże się z tym że trzeba będzie obsłużyć przekazywanie dźwięków do aplikacji, a później zająć się ich odpowiednią obróbką. Konieczne będzie jednak wtedy przekształcanie muzyki stereo do kanału mono.
	- API dla aplikacji:
		- aplikacja typu web-service (MVC,.NET CORE WEB API,Razor Pages, Blazor)
			- MVC raczej będzie słabe bo nie zakładamy wielu interfejsów graficznych czy podstron
			- Razor Pages redukuje potrzebę tworzenia kontrolerów, a całość aplikacji dzielimy na strony (poza tym bardzo lekkie)
			- Blazor - fajny jak chcemy ciekawe, interaktywne API bez babrania się w JavaScripcie
		- aplikacja mobilna (MAUI) - crossplatformowość (może być wtedy na mobilki i na desktop), też sposko jeżeli chodzi o desktop, ale również wymaga uczenia się XAMLA
		- aplikacja desktopowa (WPF) - desktopówka wyłącznie pod Windows (trochę słabo bo trzeba się XAMLA uczyć u MVVM)
		
		Chyba najlepszym wyborem będą zatem RazorPages (prosty serwis internetowy w oparciu o model HTML, CSS i C#)
	
	- Pipeline dla przetwarzania utworu:
		1) pobieramy utwór od użytkownika (na początku może być on dostarczany za pomocą podania odpowiedniej ścieżki, później opracuje się algorytm do efektywnego zapisywania tymczasowo utworu w celu jego dalszego przetwarzania). Prawdopodobnie będzie do tego dedykowany folder, w którym przechowywane będą dźwięki. Dostarczane przez użytkownika pliki dźwiękowe również będą do niego trafiały. Folder służyć będzie utworzeniu od podstaw bazy danych w postaci haszy jednakże system nie oczekuje bezpośredniego dostępu do utworów. 
		2) Z dziedziny czasu za pomocą dyskretnej transformaty Fouriera przenosimy utwór do dziedziny częstotliwości
		3) W dziedzinie częstotliwości zbieramy hasze dla danego utworu
			1. Dzielimy sobie nasz utwór na czunki o stałym rozmiarze aby nie stracić informacji o czasie dla którego występują dane częstotliwości.
			2. Dla każdego chunka zachodzi konieczność wykonania FFT
		4) Dla odpowiednich zakresów częstotliwości w piosence, ustalamy sobie zakresy dla których sprawdzamy najwyższą magintude (natężenie dźwięku), a później wykonujemy sobie LUT z tych wartości dla każdej piosenki w ramch zakresów. Gdy w bazie pojawia się nowa piosenka, możemy ją zidentyfikować jako na podstawie największej ilośc prasujących to siebie magnitude z odpowiednich zakresów. Proces ten zakłada przyspieszenie działania algorytmu, ponieważ sprawdzenie wszystkich punktów dla wszystkich częstotliwości w piosence na podstawie listy chunków byłoby kłopotliwe. Iterujemy zatem po kolejnych częstotliwościach i na podstawie wartości pasujących do danego zakresu ustalamy która z wartości magnitude jest największa.
		Zapisujemy ją potem w LUT jako sposób na identyfikację piosenki. LUT możemy zapisywać w pliku z usystematyzowanym dla nas formatem zapisu tak by dało się potem te wartości jednoznacznie odczytywać.
		5) Gdy mamy już wartości magnitude dla odpowiednich zakresów częstotliwości, możemy przystąpić do tworzenia hasza. Hasz tworzymy za pomocą wartości uzyskanych z odpowiednich zakresów podstawiając pod to adekwatny wzór który utworzy nam pewną wartość liczbową. Podział na zakresy częstotliwości i generowanie haszy wykonujemy dla każdej linii analizy spektralnej.
		6a) UTWÓR DO UZUPEŁNIENIA BAZY: jeżeli dodajemy utwór, to podawana jest również informacja o jego tytule, autorze, długości, gatunku etc., potem do tabeli z haszami dorzucana jest informacja na temat haszu
		6b) UTWÓR DO ROZPOZNANIA - nie mamy informacji dodatkowych a zatem pozyskane hasze spawdzamy pod kątem największej liczby dopasowań z haszami innych utworów. Utwór o największej liczbie dopasowań będzie jako wynik wyszukiwania
		
		W opisie projekowania Shazam, ostatnia sekcja mówi o tym że tworzymy 2 data sety. Pierwszy przechowuje kluczowe informacje o utworach a zatem możemy tam zamieścić tytuł, autora, czas trwania ewentualnie gatunek.
		
		Drugi dataset służyć będzie do wykrywania piosenek. A mianowicie Będzie to ogromna baza z haszami gdzie do każdego hasza przypisana będzie lista punktów w uworach (DataPoints). DataPoint składać się będzie z id utworu oraz czasu w którym dany hasz występuje w utworze.
		
		Celem wykrycia utworu będzie zatem wykorzystanie haszy z aktualnie przetwarzanej piosenki i sprawdzenie czy w bazie nie ma już danych haszy, a jeśli są to jak wyglądają offsety w czasie? (nie musimy przecież dysponować nagraniem piosenki od początku do końca). Może to być coś w trakcie. Na koniec przetwarzania powinniśmy mieć dość sporą listę pasujących haszy z różnych piosenek z różnymi offestami. Kluczowym będzie zatem weryfikacja ile razy dane ID się powtarza oraz czy offsety +- pokrywają się ze sobą (znaleźć na to jakiś skuteczny algorytm lub wartość statystyczną np. odchylenie standardowe)
		
		Generalnie zatem ne przywyczajamy się że hasz przynależy do konkretnej piosenki lecz raczej że piosenka posiada konkretnego hasza w danym punkcie utworu (ale może posiadać podobnego hasza również inna piosenka).
		
		Na dobry początek dobrze by było wybrać 10 losowych utworów (small data set) i stworzyć pipeline odpowiedzialny za faktyczne identyfikowanie piosenek i sprawdzić czy działa. Przejść sobie przez te wszystkie kroki i zobaczyć czy algorytm jest słuszny i jak będzie wyglądała jego implementacja.
		
		Ogarnąć sobie jakiś .gitignore pod aplikację konsolową w C# żeby śmieci nie wrzucało na gita.
		
		W kontekście niejasności związanych z generowaniem wartości dla linii widma spektralnego, działa to mniej więcej w ten sposób, że my najpierw zapisujemy sobie wszystkie częstotliwości naszego utworu jako tablice wartości Complex w ramach poszczególnych chunków. Moglibyśmy zrobić pojedynczą tablicę Complex, ale wtedy utracilibyśmy informacje w którym miejscu w utworze pojawiają się dane częstotliwości (tak to wiemy że w obrębie CHUNK_SIZE, występuje dana częstotliwość, czyli wiemy który to jest chunk utworu)
		
		Dalej natomiast traktujemy już nasze częstotliwości jak jedną tablicę, a zatem musimy przeiterować po każdym chunku, sprawdzając odpowiednie zakresy częstolitowści (iterujemy po 2 indeksie tablicy result żeby przesuwać się po kolejnych częstotliwościach) i dla zakresów wybieramy największe wartości magnitude.
		
		Oznacza to mniej więcej tyle, że w każdym chunku powinniśmy dostać komplet 5 najwyższych wartości magnitude w ramach danego chunka. Ponieważ powtarzamy czynność dla każdego chunka, to tych wartości będzie dużo.
		
		Zbieramy zatem informację
		"Ok, dla każdej linii w pliku (chunka), wiemy że dla tego chunka to są najwyższe wartości natężenia dźwięku w konrketnych zakresach częstotliwości"
		
		PYTANIE:
		Chociaż w naszym algorytmie wszystko się sprwdza do momentu pobrania częstotliwości wciąż pozostaje pytanie w jaki sposób pozyskac wartość magnitude dźwięku (wartość double) mając do dyspozycji wartość complex i wzór jak w przykładzie.
		
		ODPOWIEDŹ: Żeby przkształcić wartość Complex do wartości decimal, wykorzystujemy w tym celu Abs dla liczb zespolonych co jest inną kwestią niż abs dla liczb numerycznych.
		
		Wybieranie konkretnych czestotliwości na intensywności dźwięku w danym przedziale może być jedną z metod ale nie jedyną, generalnie chodzi o to żeby zrobić w miarę sensownego hasza.
		Przedziały również nie muszą być tutaj takie same jak w przykładzie. Chodzi o dobranie dobrego kryterium odróżniania od siebie piosenek, które możemy ustalić na podstawie częstotliwości.
		
		PYTANIE:
		Czy hasze generowane są dla całej piosenki na podstawie chunków czy chunki to coś zupełnie odrębnego w tym momencie i jedyne co nas int. Warto zwrócić uwagę na to że w wideo gościu mówi o czymś zupełnie innym niż pisze a blogu. Tam selekcja dokonywana jest nieco inaczej.
		
		Nas, w całym utworze interesują generalnie tylko wartości największe z przedziałów (tych przedziałów może być tyle ile chcemy i w miejscach takie jakie chcemy).
		Chodzi o to że wartości maksymalne z przedziałów aplikują się dla CAŁEJ PIOSENKI (a zatem jak zdecydujemy się na przykład na 5 przedziałów, to w naturalny sposób skończymy z piosenką opisaną za pomocą 5 liczb).
		
		PYTANIE:
			Skoro i tak hasz jest generowany dla całej piosenki, to po co w takim razie nam jest podział na chunki? Moim zdaniem hasze powinny być generowane dla każdego chunka w piosence. W ten sposób jesteśmy w stanie faktycznie uzyskać znaczącą liczbę dopasowań dla piosenki. Obejrzeć sobie jeszcze raz film tego gościa chociaż moim zdaniem najwięcej sensu ma teoria, że każdy chunk ma swojego hasza. Wtedy mamy pole do popisu żeby porównywać. Pytanie czy nie za duży nakład obliczeeniowy przy porównywaniu haszy?
			
			Jeżeli chcemy żeby zapisywało hasze dla każdego chunka piosenki, to konieczne jest stworzenie w tym momencie bazy danych, która będzie przechowywała dane o haszach oraz funkcji która będzie czyściła bazę przy nowym generowaniu haszy dla piosenek / generowała hasze tylko dla nowo dodanych piosenek (tytuł nie pojwił się w bazie).
			
		POMYSŁ
			Aby system sam zwracał wynik po pewnym czasie, można ustawić limit w postaci ilości dopasowań. Po osiągnięciu konkrentej liczby dopasowań (ustalane na sztywno) dla którejś piosenki, program automatycznie zwróci ją jako wynik. Trzeba jednak zbilansować czas oczekiwania, do celności udzielanych odpowiedzi.
			Drugim kryterium ograniczającym powinien być czas oczekiwania na wynik. Po jego upływie, program zwraca komuniakt o tym że nie udało się rozpoznać utworu (dopasować na podstawie szybkości odszukiwania piosenki)
			
		Projekt nad którym obecnie pracuję jest programem do generowania haszy
		Powstanie osobny program dedykowany do sprawdzania haszy (jeszcze nie wiem za bardzo jak). Nie wiem tylko jeszcze czy tworzyć 3 program obsługujący API aplikacji czy lepiej połączyć sprawdzanie haszy, upload do bazy i MVC jako jedno.
		
		
		
		Alternatywy dla Shazama:
			- Midomi
			- SoundHound
			- Musicxmatch
			- Beatfind
			- Soly
			- MusicID
			- Google Assistant
		Biblioteki do fingerprintingu pozwalające na tworzenie własnych bibliotek detekcyjnych:
			- dejavu (Python)
			- Ecoprint
			- Chromaprint
			
		Wczytywanie dźwięku z mikrofonu odbędzie się za pomocą WaveInEvent. Teoretycznie wymagane jest żeby rozpocząć i zakończyć takie nagrywanie a zatem nastąpi to po zadanym upływie czasu (np. 20 sekundach), albo gdy znaleziony ostatnie prawidłowy kandydat.
		
		
		
		
		
		
		
